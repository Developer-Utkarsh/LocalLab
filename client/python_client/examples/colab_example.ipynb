{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LocalLab Client Example\n",
        "\n",
        "This notebook demonstrates how to use the LocalLab client to connect to a running LocalLab server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Install the client package\n",
        "!pip install locallab-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note: Make sure you have a LocalLab server running!\n",
        "You need to:\n",
        "1. Open another Colab notebook\n",
        "2. Install LocalLab server: `pip install locallab`\n",
        "3. Start the server with ngrok: \n",
        "```python\n",
        "import os\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"your_token\"\n",
        "from locallab import start_server\n",
        "start_server(use_ngrok=True)\n",
        "```\n",
        "4. Copy the ngrok URL provided by the server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import asyncio\n",
        "from locallab import LocalLabClient\n",
        "\n",
        "# Replace with your LocalLab server's ngrok URL\n",
        "SERVER_URL = \"https://xxxx-xx-xx-xxx-xx.ngrok.io\"  # Get this from the server notebook\n",
        "\n",
        "# Initialize client\n",
        "client = LocalLabClient({\n",
        "    \"base_url\": SERVER_URL,\n",
        "    \"timeout\": 30.0\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Check if server is running\n",
        "async def check_connection():\n",
        "    is_healthy = await client.health_check()\n",
        "    print(f\"Server connection: {'OK' if is_healthy else 'Failed'}\")\n",
        "\n",
        "await check_connection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Get available models from server\n",
        "async def check_models():\n",
        "    models = await client.list_models()\n",
        "    print(\"Available models on server:\")\n",
        "    for model_id, info in models.items():\n",
        "        print(f\"- {model_id}: {info.description}\")\n",
        "\n",
        "await check_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Basic text generation\n",
        "async def generate_text():\n",
        "    response = await client.generate(\n",
        "        \"Tell me a story about a robot learning to paint\",\n",
        "        {\"temperature\": 0.7}\n",
        "    )\n",
        "    print(\"AI Response:\")\n",
        "    print(response.response)\n",
        "\n",
        "await generate_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Chat with the AI\n",
        "async def chat_with_ai():\n",
        "    response = await client.chat([\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ])\n",
        "    print(\"AI Response:\")\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "await chat_with_ai()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Stream responses in real-time\n",
        "async def stream_response():\n",
        "    print(\"AI is thinking...\")\n",
        "    async for token in client.stream_generate(\"Once upon a time\"):\n",
        "        print(token, end=\"\", flush=True)\n",
        "    print()\n",
        "\n",
        "await stream_response()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Multiple questions at once\n",
        "async def batch_questions():\n",
        "    questions = [\n",
        "        \"What is 2+2?\",\n",
        "        \"Who wrote Romeo and Juliet?\",\n",
        "        \"What is the speed of light?\"\n",
        "    ]\n",
        "    \n",
        "    responses = await client.batch_generate(questions)\n",
        "    \n",
        "    print(\"Batch Responses:\")\n",
        "    for q, a in zip(questions, responses.responses):\n",
        "        print(f\"\\nQ: {q}\")\n",
        "        print(f\"A: {a}\")\n",
        "\n",
        "await batch_questions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Check server status\n",
        "async def check_server_status():\n",
        "    info = await client.get_system_info()\n",
        "    print(\"Server Status:\")\n",
        "    print(f\"CPU Usage: {info.cpu_usage}%\")\n",
        "    print(f\"Memory Usage: {info.memory_usage}%\")\n",
        "    if info.gpu_info:\n",
        "        print(f\"GPU: {info.gpu_info.device}\")\n",
        "    print(f\"Active Model: {info.active_model}\")\n",
        "\n",
        "await check_server_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Clean up\n",
        "await client.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
