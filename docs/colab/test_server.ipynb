{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LocalLab Server Test Notebook\n",
        "\n",
        "This notebook helps test the LocalLab server package on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's make sure we have a clean environment\n",
        "!pip uninstall -y locallab\n",
        "!pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies first\n",
        "!pip install --upgrade transformers accelerate\n",
        "!pip install torch pyngrok fastapi uvicorn huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the package from TestPyPI\n",
        "!pip install --no-cache-dir --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ locallab==0.1.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Set your ngrok token\n",
        "NGROK_TOKEN = input(\"Enter your ngrok token: \").strip()\n",
        "if not NGROK_TOKEN:\n",
        "    raise ValueError(\"Ngrok token is required for running the server\")\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = NGROK_TOKEN\n",
        "\n",
        "# Choose model configuration\n",
        "print(\"\\nAvailable default models:\")\n",
        "print(\"1. microsoft/phi-2 (Default, 2.7B parameters)\")\n",
        "print(\"2. TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B parameters)\")\n",
        "print(\"3. stabilityai/stable-code-3b (3B parameters)\")\n",
        "print(\"4. Custom model from Hugging Face\")\n",
        "\n",
        "choice = input(\"\\nChoose model (1-4, default is 1): \").strip() or \"1\"\n",
        "\n",
        "if choice == \"4\":\n",
        "    custom_model = input(\"Enter Hugging Face model ID (e.g., meta-llama/Llama-3.2-3B-Instruct): \").strip()\n",
        "    if not custom_model:\n",
        "        raise ValueError(\"Custom model ID is required when choosing option 4\")\n",
        "    os.environ[\"LOCALLAB_CUSTOM_MODEL\"] = custom_model\n",
        "    os.environ[\"LOCALLAB_DEFAULT_MODEL\"] = custom_model.split(\"/\")[-1]\n",
        "else:\n",
        "    models = {\n",
        "        \"1\": \"phi-2\",\n",
        "        \"2\": \"tinyllama-1.1b\",\n",
        "        \"3\": \"stable-code-3b\"\n",
        "    }\n",
        "    os.environ[\"LOCALLAB_DEFAULT_MODEL\"] = models.get(choice, \"phi-2\")\n",
        "\n",
        "# Configure performance settings\n",
        "os.environ[\"LOCALLAB_ENABLE_FLASH_ATTENTION\"] = \"true\"\n",
        "os.environ[\"LOCALLAB_ENABLE_ATTENTION_SLICING\"] = \"true\"\n",
        "os.environ[\"LOCALLAB_ENABLE_CPU_OFFLOADING\"] = \"true\"\n",
        "os.environ[\"LOCALLAB_ENABLE_BETTERTRANSFORMER\"] = \"true\"\n",
        "os.environ[\"LOCALLAB_ENABLE_QUANTIZATION\"] = \"true\"\n",
        "os.environ[\"LOCALLAB_QUANTIZATION_TYPE\"] = \"int8\"\n",
        "os.environ[\"LOCALLAB_MIN_FREE_MEMORY\"] = \"2000\"\n",
        "\n",
        "# Configure server settings\n",
        "os.environ[\"LOCALLAB_MAX_CONCURRENT_REQUESTS\"] = \"10\"\n",
        "os.environ[\"LOCALLAB_ENABLE_DYNAMIC_BATCHING\"] = \"true\"\n",
        "os.environ[\"LOCALLAB_BATCH_TIMEOUT\"] = \"100\"\n",
        "os.environ[\"LOCALLAB_ENABLE_CACHE\"] = \"true\"\n",
        "os.environ[\"LOCALLAB_CACHE_TTL\"] = \"3600\"\n",
        "\n",
        "logging.info(f\"Using model: {os.environ.get('LOCALLAB_DEFAULT_MODEL')}\")\n",
        "logging.info(f\"Flash Attention: {os.environ.get('LOCALLAB_ENABLE_FLASH_ATTENTION')}\")\n",
        "logging.info(f\"Quantization: {os.environ.get('LOCALLAB_QUANTIZATION_TYPE')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and start the server\n",
        "try:\n",
        "    from locallab import start_server, MODEL_REGISTRY, can_run_model\n",
        "    print(\"Successfully imported locallab\")\n",
        "    \n",
        "    # Check if model can run\n",
        "    model_id = os.environ.get(\"LOCALLAB_DEFAULT_MODEL\", \"phi-2\")\n",
        "    if not can_run_model(model_id):\n",
        "        print(f\"Warning: {model_id} may not run optimally on current resources\")\n",
        "        print(\"Available models:\")\n",
        "        for model in MODEL_REGISTRY:\n",
        "            if can_run_model(model):\n",
        "                print(f\"- {model}: {MODEL_REGISTRY[model]['description']}\")\n",
        "        use_model = input(\"Choose a different model or press Enter to continue anyway: \").strip()\n",
        "        if use_model:\n",
        "            os.environ[\"LOCALLAB_DEFAULT_MODEL\"] = use_model\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"\\nTrying to find the module:\")\n",
        "    !find /usr/local/lib/python3.* -name \"locallab*\"\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the server\n",
        "try:\n",
        "    start_server(use_ngrok=True)\n",
        "except ValueError as e:\n",
        "    if \"NGROK_AUTH_TOKEN\" in str(e):\n",
        "        print(\"Error: Please set your ngrok token in the cell above\")\n",
        "    else:\n",
        "        print(f\"Configuration error: {str(e)}\")\n",
        "        print(\"\\nPlease check your environment variables and try again\")\n",
        "except Exception as e:\n",
        "    print(f\"Error starting server: {str(e)}\")\n",
        "    print(\"\\nTrying to fall back to default model...\")\n",
        "    try:\n",
        "        os.environ[\"LOCALLAB_DEFAULT_MODEL\"] = \"phi-2\"\n",
        "        os.environ[\"LOCALLAB_ENABLE_QUANTIZATION\"] = \"true\"\n",
        "        os.environ[\"LOCALLAB_QUANTIZATION_TYPE\"] = \"int8\"\n",
        "        start_server(use_ngrok=True)\n",
        "    except Exception as e2:\n",
        "        print(f\"Fallback also failed: {str(e2)}\")\n",
        "        print(\"\\nPlease try the following:\")\n",
        "        print(\"1. Restart the runtime\")\n",
        "        print(\"2. Check your internet connection\")\n",
        "        print(\"3. Verify your ngrok token\")\n",
        "        print(\"4. Try a smaller model\")\n",
        "        raise"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LocalLab Server Test",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
